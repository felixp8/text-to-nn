# text-to-nn
playing around with generating neural network parameters from text

## experiments

### mlp

idea: train mlps on randomly generated functions. then, train generative model to generate mlp parameters, conditioned on text embeddings of said functions. and maybe that will work

### rnn

maybe one day? need to be able to compose and generate rnn tasks, with corresponding natural language task descriptions, then train networks on them. tasks can be something akin to mod-cog.