{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puK8lgrbVG0t"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/felixp8/text-to-nn/blob/main/experiments/mlp/diffusion/diffusion.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/felixp8/text-to-nn.git"
      ],
      "metadata": {
        "id": "h2Eyr_JycfCL",
        "outputId": "a3b4ef46-0d66-4283-a85f-5b09feba79c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'text-to-nn'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 184 (delta 29), reused 28 (delta 9), pack-reused 106\u001b[K\n",
            "Receiving objects: 100% (184/184), 71.23 MiB | 21.25 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4zvHF7y8VG0u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QAkjI_llVG0v"
      },
      "outputs": [],
      "source": [
        "class GaussianFourierProjection(nn.Module):\n",
        "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
        "    def __init__(self, embed_dim, scale=30.):\n",
        "        super().__init__()\n",
        "        # Randomly sample weights during initialization. These weights are fixed\n",
        "        # during optimization and are not trainable.\n",
        "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
        "    def forward(self, x):\n",
        "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
        "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScoreNet(nn.Module):\n",
        "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
        "\n",
        "    def __init__(self, marginal_prob_std, input_dim, hidden_dims=[], embed_dim=256, context_dim=768):\n",
        "        \"\"\"Initialize a time-dependent score-based network.\n",
        "\n",
        "        Args:\n",
        "          marginal_prob_std: A function that takes time t and gives the standard\n",
        "            deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
        "          channels: The number of channels for feature maps of each resolution.\n",
        "          embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Gaussian random feature embedding layer for time\n",
        "        self.t_embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
        "            nn.Linear(embed_dim, embed_dim))\n",
        "        self.y_embed = nn.Linear(context_dim, embed_dim)\n",
        "        # Encoding layers where the resolution decreases\n",
        "        hidden_dims = [input_dim,] + hidden_dims + [input_dim,]\n",
        "        x_layers = []\n",
        "        t_layers = []\n",
        "        norm_layers = []\n",
        "        for i in range(len(hidden_dims) - 2):\n",
        "            x_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
        "            t_layers.append(nn.Linear(embed_dim, hidden_dims[i+1]))\n",
        "            norm_layers.append(nn.LayerNorm(hidden_dims[i+1]))\n",
        "        self.x_layers = nn.ModuleList(x_layers)\n",
        "        self.t_layers = nn.ModuleList(t_layers)\n",
        "        self.norm_layers = nn.ModuleList(norm_layers)\n",
        "        self.final = nn.Linear(hidden_dims[-2], hidden_dims[-1])\n",
        "\n",
        "        # The swish activation function\n",
        "        self.act = nn.SiLU()\n",
        "        self.marginal_prob_std = marginal_prob_std\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Obtain the Gaussian random feature embedding for t\n",
        "        t_embed = self.act(self.t_embed(t))\n",
        "\n",
        "        h = x\n",
        "        for i in range(len(self.x_layers)):\n",
        "            h = self.x_layers[i](h)\n",
        "            h += self.t_layers[i](t_embed)\n",
        "            h = self.norm_layers[i](h)\n",
        "            h = self.act(h)\n",
        "\n",
        "        h = self.act(self.final(h))\n",
        "\n",
        "        # Normalize output\n",
        "        h = h / self.marginal_prob_std(t)[:, None]\n",
        "        return h\n",
        "\n",
        "\n",
        "class ScoreNetConditional(nn.Module):\n",
        "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
        "\n",
        "    def __init__(self, marginal_prob_std, input_dim, hidden_dims=[], embed_dim=256, context_dim=768):\n",
        "        \"\"\"Initialize a time-dependent score-based network.\n",
        "\n",
        "        Args:\n",
        "          marginal_prob_std: A function that takes time t and gives the standard\n",
        "            deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
        "          channels: The number of channels for feature maps of each resolution.\n",
        "          embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Gaussian random feature embedding layer for time\n",
        "        self.t_embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
        "            nn.Linear(embed_dim, embed_dim))\n",
        "        self.y_embed = nn.Linear(context_dim, embed_dim)\n",
        "        # Encoding layers where the resolution decreases\n",
        "        hidden_dims = [input_dim,] + hidden_dims + [input_dim,]\n",
        "        x_layers = []\n",
        "        t_layers = []\n",
        "        y_layers = []\n",
        "        norm_layers = []\n",
        "        for i in range(len(hidden_dims) - 2):\n",
        "            x_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
        "            t_layers.append(nn.Linear(embed_dim, hidden_dims[i+1]))\n",
        "            y_layers.append(nn.Linear(embed_dim, hidden_dims[i+1]))\n",
        "            norm_layers.append(nn.LayerNorm(hidden_dims[i+1]))\n",
        "        self.x_layers = nn.ModuleList(x_layers)\n",
        "        self.t_layers = nn.ModuleList(t_layers)\n",
        "        self.y_layers = nn.ModuleList(y_layers)\n",
        "        self.norm_layers = nn.ModuleList(norm_layers)\n",
        "        self.final = nn.Linear(hidden_dims[-2], hidden_dims[-1])\n",
        "\n",
        "        # The swish activation function\n",
        "        self.act = nn.SiLU()\n",
        "        self.marginal_prob_std = marginal_prob_std\n",
        "\n",
        "    def forward(self, x, t, y):\n",
        "        # Obtain the Gaussian random feature embedding for t\n",
        "        t_embed = self.act(self.t_embed(t))\n",
        "        y_embed = self.act(self.y_embed(y))\n",
        "\n",
        "        h = x\n",
        "        for i in range(len(self.x_layers)):\n",
        "            h = self.x_layers[i](h)\n",
        "            h += self.t_layers[i](t_embed)\n",
        "            h += self.y_layers[i](y_embed)\n",
        "            h = self.norm_layers[i](h)\n",
        "            h = self.act(h)\n",
        "\n",
        "        h = self.act(self.final(h))\n",
        "\n",
        "        # Normalize output\n",
        "        h = h / self.marginal_prob_std(t)[:, None]\n",
        "        return h"
      ],
      "metadata": {
        "id": "URuWfB9-XTdb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu' # ['cuda', 'cpu']\n",
        "\n",
        "def marginal_prob_std(t, sigma):\n",
        "    \"\"\"Compute the mean and standard deviation of $p_{0t}(x(t) | x(0))$.\n",
        "\n",
        "    Args:\n",
        "      t: A vector of time steps.\n",
        "      sigma: The $\\sigma$ in our SDE.\n",
        "\n",
        "    Returns:\n",
        "      The standard deviation.\n",
        "    \"\"\"\n",
        "    t = torch.tensor(t, device=device)\n",
        "    return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
        "\n",
        "def diffusion_coeff(t, sigma):\n",
        "    \"\"\"Compute the diffusion coefficient of our SDE.\n",
        "\n",
        "    Args:\n",
        "      t: A vector of time steps.\n",
        "      sigma: The $\\sigma$ in our SDE.\n",
        "\n",
        "    Returns:\n",
        "      The vector of diffusion coefficients.\n",
        "    \"\"\"\n",
        "    return torch.tensor(sigma**t, device=device)\n",
        "\n",
        "sigma =  50.0\n",
        "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
        "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)"
      ],
      "metadata": {
        "id": "4H6k1xBcahV2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(model, x, marginal_prob_std, y=None, eps=1e-5):\n",
        "    \"\"\"The loss function for training score-based generative models.\n",
        "\n",
        "    Args:\n",
        "      model: A PyTorch model instance that represents a\n",
        "        time-dependent score-based model.\n",
        "      x: A mini-batch of training data.\n",
        "      marginal_prob_std: A function that gives the standard deviation of\n",
        "        the perturbation kernel.\n",
        "      eps: A tolerance value for numerical stability.\n",
        "    \"\"\"\n",
        "    random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps\n",
        "    z = torch.randn_like(x)\n",
        "    std = marginal_prob_std(random_t)\n",
        "    perturbed_x = x + z * std[:, None]\n",
        "    if y is not None:\n",
        "        score = model(perturbed_x, random_t, y)\n",
        "    else:\n",
        "        score = model(perturbed_x, random_t)\n",
        "    loss = torch.mean(torch.sum((score * std[:, None] + z)**2, dim=(1,)))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "6x2AtDmcdq49"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import h5py\n",
        "\n",
        "expressions_file = \"./text-to-nn/experiments/mlp/data_generation/data/tiny/expressions.csv\"\n",
        "parameters_file = \"./text-to-nn/experiments/mlp/data_generation/data/tiny/parameters.h5\"\n",
        "embeddings_file = \"./text-to-nn/experiments/mlp/data_generation/data/tiny/instructor_embeddings.h5\"\n",
        "\n",
        "expr_csv = pd.read_csv(expressions_file)\n",
        "with h5py.File(parameters_file, 'r') as h5f:\n",
        "    parameters = h5f['nn_parameters'][:h5f['counter'][()].item()]\n",
        "with h5py.File(embeddings_file, 'r') as h5f:\n",
        "    embeddings = h5f['embeddings'][()]\n",
        "\n",
        "assert expr_csv.shape[0] == parameters.shape[0]\n",
        "assert expr_csv.shape[0] == embeddings.shape[0]\n",
        "\n",
        "if True:\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    manual_embeddings = OneHotEncoder(sparse_output=False).fit_transform(np.array(expr_csv.expr)[:, None])\n",
        "    embeddings = manual_embeddings"
      ],
      "metadata": {
        "id": "nfRYnlZXp43_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_mask = (expr_csv['best_mse_loss'] < 1.)\n",
        "\n",
        "expr_csv = expr_csv[good_mask]\n",
        "parameters = parameters[good_mask]\n",
        "embeddings = embeddings[good_mask]"
      ],
      "metadata": {
        "id": "rRwJ-jBMqdkU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask = np.random.choice(parameters.shape[0], size=int(parameters.shape[0]*0.6), replace=False)\n",
        "train_mask = np.isin(np.arange(parameters.shape[0]), train_mask)\n",
        "valid_mask = ~train_mask\n",
        "\n",
        "train_expr = expr_csv[train_mask]\n",
        "train_parameters = parameters[train_mask]\n",
        "train_embeddings = embeddings[train_mask]\n",
        "\n",
        "valid_expr = expr_csv[valid_mask]\n",
        "valid_parameters = parameters[valid_mask]\n",
        "valid_embeddings = embeddings[valid_mask]"
      ],
      "metadata": {
        "id": "A5brjqqNquX4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_parameters = torch.from_numpy(train_parameters).to(torch.float)\n",
        "train_embeddings = torch.from_numpy(train_embeddings).to(torch.float)"
      ],
      "metadata": {
        "id": "ntmBj0YYr90A"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "score_model = ScoreNetConditional(\n",
        "    input_dim=65,\n",
        "    hidden_dims=[128, 64, 32, 64, 128],\n",
        "    embed_dim=128,\n",
        "    context_dim=8,\n",
        "    marginal_prob_std=marginal_prob_std_fn,\n",
        ")\n",
        "score_model = score_model.to(device)\n",
        "\n",
        "n_epochs = 100\n",
        "## size of a mini-batch\n",
        "batch_size =  256\n",
        "## learning rate\n",
        "lr=1e-3\n",
        "## log freq\n",
        "log_freq = 10\n",
        "\n",
        "dataset = TensorDataset(train_parameters, train_embeddings)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "optimizer = optim.Adam(score_model.parameters(), lr=lr)\n",
        "for epoch in range(n_epochs):\n",
        "    avg_loss = 0.\n",
        "    num_items = 0\n",
        "    for x, y in data_loader:\n",
        "        x = x.to(device)\n",
        "        loss = loss_fn(score_model, x, marginal_prob_std_fn, y)\n",
        "        # loss = loss_fn(score_model, x, marginal_prob_std_fn)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item() * x.shape[0]\n",
        "        num_items += x.shape[0]\n",
        "    # Print the averaged training loss so far.\n",
        "    if epoch % log_freq == 0:\n",
        "        print('Epoch {:04d} Average Loss: {:5f}'.format(epoch, avg_loss / num_items))\n",
        "        # Update the checkpoint after each epoch of training.\n",
        "        torch.save(score_model.state_dict(), 'ckpt.pth')"
      ],
      "metadata": {
        "id": "fluo48Msd5bA",
        "outputId": "5d672f20-2a84-46ff-fe8e-2846aa5157b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "<ipython-input-5-b1262f4ee541>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  t = torch.tensor(t, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0000 Average Loss: 64.284260\n",
            "Epoch 0010 Average Loss: 49.406865\n",
            "Epoch 0020 Average Loss: 47.394924\n",
            "Epoch 0030 Average Loss: 47.193865\n",
            "Epoch 0040 Average Loss: 46.446875\n",
            "Epoch 0050 Average Loss: 46.043321\n",
            "Epoch 0060 Average Loss: 45.320599\n",
            "Epoch 0070 Average Loss: 45.278607\n",
            "Epoch 0080 Average Loss: 44.911371\n",
            "Epoch 0090 Average Loss: 44.357430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps =  500\n",
        "def Euler_Maruyama_sampler(score_model,\n",
        "                           input_dim,\n",
        "                           y,\n",
        "                           marginal_prob_std,\n",
        "                           diffusion_coeff,\n",
        "                           batch_size=64,\n",
        "                           num_steps=num_steps,\n",
        "                           device='cuda',\n",
        "                           eps=1e-3):\n",
        "    \"\"\"Generate samples from score-based models with the Euler-Maruyama solver.\n",
        "\n",
        "    Args:\n",
        "      score_model: A PyTorch model that represents the time-dependent score-based model.\n",
        "      marginal_prob_std: A function that gives the standard deviation of\n",
        "        the perturbation kernel.\n",
        "      diffusion_coeff: A function that gives the diffusion coefficient of the SDE.\n",
        "      batch_size: The number of samplers to generate by calling this function once.\n",
        "      num_steps: The number of sampling steps.\n",
        "        Equivalent to the number of discretized time steps.\n",
        "      device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
        "      eps: The smallest time step for numerical stability.\n",
        "\n",
        "    Returns:\n",
        "      Samples.\n",
        "    \"\"\"\n",
        "    t = torch.ones(batch_size, device=device)\n",
        "    init_x = torch.randn(batch_size, input_dim, device=device) \\\n",
        "        * marginal_prob_std(t)[:, None]\n",
        "    time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "    x = init_x\n",
        "    with torch.no_grad():\n",
        "        for time_step in time_steps:\n",
        "            batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
        "            g = diffusion_coeff(batch_time_step)\n",
        "            mean_x = x + (g**2)[:, None] * score_model(x, batch_time_step, y) * step_size\n",
        "            x = mean_x + torch.sqrt(step_size) * g[:, None] * torch.randn_like(x)\n",
        "    # Do not include any noise in the last sampling step.\n",
        "    return mean_x"
      ],
      "metadata": {
        "id": "I2mWdjeDr6w5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the pre-trained checkpoint from disk.\n",
        "device = 'cpu' # ['cuda', 'cpu']\n",
        "ckpt = torch.load('ckpt.pth', map_location=device)\n",
        "score_model.load_state_dict(ckpt)\n",
        "\n",
        "sample_batch_size = 64\n",
        "sampler = Euler_Maruyama_sampler\n",
        "y = torch.from_numpy(valid_embeddings[:sample_batch_size]).to(torch.float)\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples = sampler(score_model,\n",
        "                  65,\n",
        "                  y,\n",
        "                  marginal_prob_std_fn,\n",
        "                  diffusion_coeff_fn,\n",
        "                  sample_batch_size,\n",
        "                  device=device)"
      ],
      "metadata": {
        "id": "DeBfU62rtnAc",
        "outputId": "136ef37f-0ca0-4720-e067-119b34236f86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b1262f4ee541>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  t = torch.tensor(t, device=device)\n",
            "<ipython-input-5-b1262f4ee541>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(sigma**t, device=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples.shape"
      ],
      "metadata": {
        "id": "-fdaOl9luHyx",
        "outputId": "52c2150b-44f6-4720-876b-8ff8d52ff7cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int, activation: str = \"relu\", bias=True):\n",
        "        super().__init__()\n",
        "        dims = [input_dim] + hidden_dims + [output_dim]\n",
        "\n",
        "        if activation == \"relu\":\n",
        "            activation = nn.ReLU\n",
        "        elif activation == \"sigmoid\":\n",
        "            activation = nn.Sigmoid\n",
        "        elif activation == \"tanh\":\n",
        "            activation = nn.Tanh\n",
        "        elif activation == \"gelu\":\n",
        "            activation = nn.GELU\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "        layerlist = []\n",
        "        for i in range(len(dims) - 2):\n",
        "            layerlist.append(nn.Linear(dims[i], dims[i+1], bias=bias))\n",
        "            layerlist.append(activation())\n",
        "        layerlist.append(nn.Linear(dims[-2], dims[-1], bias=True))\n",
        "\n",
        "        self.layers = nn.Sequential(*layerlist)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "5ymtNS7guZdt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_model = MLP(input_dim=2, hidden_dims=[16,], output_dim=1)\n",
        "nn.utils.vector_to_parameters(samples[0, :], sampled_model.parameters())"
      ],
      "metadata": {
        "id": "6jElt495ulUT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_expr.iloc[0]"
      ],
      "metadata": {
        "id": "XikBqruVu7LC",
        "outputId": "cf03d764-f392-4129-cea5-861f47ce3b09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0                      1\n",
              "expr                    (i1 - i0)\n",
              "index                           1\n",
              "best_mse_loss            0.000099\n",
              "best_scaled_mse_loss     0.000005\n",
              "Name: 1, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_model(torch.tensor([[1., 1.]]))"
      ],
      "metadata": {
        "id": "g3wEhrR4u9nR",
        "outputId": "40694f21-eadb-412d-dca8-aebdf56e116b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2574.7905]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FarXcuwJk_QH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}