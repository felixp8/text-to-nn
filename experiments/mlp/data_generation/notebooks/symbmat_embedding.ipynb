{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/felixp8/text-to-nn/blob/main/experiments/mlp/data_generation/notebooks/symbmat_embedding.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIT_PAT = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://felixp8:$GIT_PAT@github.com/felixp8/text-to-nn.git\n",
    "!git clone https://github.com/facebookresearch/SymbolicMathematics.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"./text-to-nn/experiments/mlp/data_generation\")\n",
    "\n",
    "expression_file = \"./data/normal/expressions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from expr_utils import *\n",
    "\n",
    "expr_csv = pd.read_csv(expression_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_list = expr_csv['expr']\n",
    "expr_list = [map_inputs(expr, 3, ['x', 'y', 'z']) for expr in expr_list]\n",
    "expr_list = [clean_expr(expr) for expr in expr_list]\n",
    "[validate_expr(expr) for expr in expr_list]\n",
    "expr_list = [sp.core.sympify(expr) for expr in expr_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "os.chdir('../../../../SymbolicMathematics/')\n",
    "\n",
    "from src.utils import AttrDict\n",
    "from src.envs import build_env\n",
    "from src.model import build_modules\n",
    "\n",
    "from src.utils import to_cuda\n",
    "from src.envs.sympy_utils import simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd_bwd.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './fwd_bwd.pth'\n",
    "assert os.path.isfile(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params = AttrDict({\n",
    "\n",
    "    # environment parameters\n",
    "    'env_name': 'char_sp',\n",
    "    'int_base': 10,\n",
    "    'balanced': False,\n",
    "    'positive': True,\n",
    "    'precision': 10,\n",
    "    'n_variables': 1,\n",
    "    'n_coefficients': 0,\n",
    "    'leaf_probs': '0.75,0,0.25,0',\n",
    "    'max_len': 512,\n",
    "    'max_int': 5,\n",
    "    'max_ops': 15,\n",
    "    'max_ops_G': 15,\n",
    "    'clean_prefix_expr': True,\n",
    "    'rewrite_functions': '',\n",
    "    'tasks': 'prim_fwd',\n",
    "    'operators': 'add:10,sub:3,mul:10,div:5,sqrt:4,pow2:4,pow3:2,pow4:1,pow5:1,ln:4,exp:4,sin:4,cos:4,tan:4,asin:1,acos:1,atan:1,sinh:1,cosh:1,tanh:1,asinh:1,acosh:1,atanh:1',\n",
    "\n",
    "    # model parameters\n",
    "    'cpu': False,\n",
    "    'emb_dim': 1024,\n",
    "    'n_enc_layers': 6,\n",
    "    'n_dec_layers': 6,\n",
    "    'n_heads': 8,\n",
    "    'dropout': 0,\n",
    "    'attention_dropout': 0,\n",
    "    'sinusoidal_embeddings': False,\n",
    "    'share_inout_emb': True,\n",
    "    'reload_model': model_path,\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = build_env(params)\n",
    "x = env.local_dict['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = build_modules(env, params)\n",
    "encoder = modules['encoder']\n",
    "decoder = modules['decoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_list = [env.sympy_to_prefix(expr) for expr in expr_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for prefix in prefix_list:\n",
    "    # x1_prefix = env.clean_prefix(['sub', 'derivative', 'f', 'x', 'x'] + x1_prefix)\n",
    "    x1 = torch.LongTensor(\n",
    "        [env.eos_index] +\n",
    "        [env.word2id[w] for w in prefix] +\n",
    "        [env.eos_index]\n",
    "    ).view(-1, 1)\n",
    "    len1 = torch.LongTensor([len(x1)])\n",
    "    x1, len1 = to_cuda(x1, len1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded = encoder('fwd', x=x1, lengths=len1, causal=False).transpose(0, 1)\n",
    "\n",
    "    embeddings.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_mode = \"mean\"\n",
    "normalize_embeddings = False\n",
    "\n",
    "if pooling_mode == \"mean\":\n",
    "    embeddings = [emb.mean(dim=(0,1)) for emb in embeddings]\n",
    "elif pooling_mode == \"last\":\n",
    "    embeddings = [emb[0,-1,:] for emb in embeddings]\n",
    "elif pooling_mode == \"first\":\n",
    "    embeddings = [emb[0,0,:] for emb in embeddings]\n",
    "embeddings = torch.stack(embeddings, dim=0)\n",
    "\n",
    "if normalize_embeddings:\n",
    "    # embeddings /= torch.nn.functional.normalize(embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./data/normal/symbmat_embeddings.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('embeddings', data=embeddings.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
